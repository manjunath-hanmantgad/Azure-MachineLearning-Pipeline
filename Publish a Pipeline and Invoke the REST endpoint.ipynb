{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8823f56d",
   "metadata": {},
   "source": [
    "Initialization Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore, Experiment, Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7045e",
   "metadata": {},
   "source": [
    "Define the datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b49fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = Datastore(ws,\"workspaceblobstore\")\n",
    "print(\"Blobstore's name is : {}\".format(def_blob_store.name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be95f0",
   "metadata": {},
   "source": [
    "Compute Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39118dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39a5c6",
   "metadata": {},
   "source": [
    "### Building Pipeline step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978f4a6",
   "metadata": {},
   "source": [
    "Step 1 : Upload data to datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626568b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = def_blob_store.upload_files([\"./20news.pkl\"], target_path=\"20newsgroups\" , overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88f3397",
   "metadata": {},
   "source": [
    "Step2 : Reference the data uploaded using from_files method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c44e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the datasource to blob_input_data variable\n",
    "\n",
    "blob_input_data = Dataset.File.from_files(data_path).as_named_input(\"Test_data\")\n",
    "print(\"Test dataset created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99912f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define intermediate data using OutputFileDatasetConfig\n",
    "\n",
    "processed_data1 = OutputFileDatasetConfig(name=\"processed_data1\")\n",
    "print(\"Output dataset object created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbe0ba",
   "metadata": {},
   "source": [
    "Define a Step that consumes a dataset and produces intermediate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trainStep consumes the datasource (Datareference) in the previous step\n",
    "# and produces processed_data\n",
    "\n",
    "source_directory = \"publish_run_train\"\n",
    "\n",
    "trainStep = PythonScriptStep(script_name=\"train.py\",\n",
    "                            arguments=[\"--input_data\" , blob_input_data.as_mount(),\n",
    "                                      \"--output_train\" , processed_data1],\n",
    "                            compute_target=aml_compute,\n",
    "                            source_directory=source_directory)\n",
    "\n",
    "print(\"Train step is created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5094c",
   "metadata": {},
   "source": [
    "Define a Step that consumes intermediate data and produces intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractStep to use the intermediate data produced by trainStep\n",
    "# This step also produces an output processed_data2\n",
    "processed_data2 = OutputFileDatasetConfig(name=\"processed_data2\")\n",
    "source_directory = \"publish_run_extract\"\n",
    "\n",
    "extractStep = PythonScriptStep(\n",
    "    script_name=\"extract.py\",\n",
    "    arguments=[\"--input_extract\", processed_data1.as_input(), \"--output_extract\", processed_data2],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory)\n",
    "print(\"extractStep created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8cd1f",
   "metadata": {},
   "source": [
    "#### PipelineParameter\n",
    "This step also has a PipelineParameter argument that help with calling the REST endpoint of the published pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_param = PipelineParameter(name=\"pipeline_arg\" , default_value=10)\n",
    "print(\"Pipeline parameter created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458efc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define compareStep that takes two inputs (both intermediate data), and produce an output\n",
    "processed_data3 = OutputFileDatasetConfig(name=\"processed_data3\")\n",
    "\n",
    "# You can register the output as dataset after job completion\n",
    "processed_data3 = processed_data3.register_on_complete(\"compare_result\")\n",
    "\n",
    "source_directory = \"publish_run_compare\"\n",
    "\n",
    "compareStep = PythonScriptStep(\n",
    "    script_name=\"compare.py\",\n",
    "    arguments=[\"--compare_data1\", processed_data1.as_input(), \"--compare_data2\", processed_data2.as_input(), \"--output_compare\", processed_data3, \"--pipeline_param\", pipeline_param],  \n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory)\n",
    "print(\"compareStep created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8b648",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=[compareStep])\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6db56",
   "metadata": {},
   "source": [
    "### Run published pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8701e41",
   "metadata": {},
   "source": [
    "Publish the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline1 = pipeline1.publish(name=\"My_New_Pipeline\", \n",
    "                                        description=\"My Published Pipeline Description\", \n",
    "                                        continue_on_step_failure=True)\n",
    "published_pipeline1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d849359",
   "metadata": {},
   "source": [
    "Publish the pipeline from a submitted PipelineRun.\n",
    "\n",
    "It is also possible to publish a pipeline from a submitted PipelineRun\n",
    "\n",
    "1. submit\n",
    "2. publish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac646f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit a pipeline run\n",
    "pipeline_run1 = Experiment(ws, 'Pipeline_experiment_sample').submit(pipeline1)\n",
    "# publish a pipeline from the submitted pipeline run\n",
    "published_pipeline2 = pipeline_run1.publish_pipeline(name=\"My_New_Pipeline2\", description=\"My Published Pipeline Description\", version=\"0.1\", continue_on_step_failure=True)\n",
    "published_pipeline2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee3c894",
   "metadata": {},
   "source": [
    "### Get published pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969d878",
   "metadata": {},
   "source": [
    "use pipeline id.\n",
    "So to get all published pipeline id use:\n",
    "all_pub_pipelines = PublishedPipeline.get_all(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "pipeline_id = published_pipeline1.id # use your published pipeline id\n",
    "published_pipeline = PublishedPipeline.get(ws, pipeline_id)\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2fe85b",
   "metadata": {},
   "source": [
    "#### Run published pipeline using its REST endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cdff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint1 = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint1))\n",
    "\n",
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint1, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"My_Pipeline1\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"ParameterAssignments\": {\"pipeline_arg\": 45}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96377cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72af25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
