{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c23d44d",
   "metadata": {},
   "source": [
    "DataTransferStep : \n",
    "\n",
    "In certain cases, you will need to transfer data from one data location to another. For example, your data may be in Azure SQL Database and you may want to move it to Azure Data Lake storage. Or, your data is in an ADLS account and you want to make it available in the Blob storage. The built-in DataTransferStep class helps you transfer data in these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff85b8",
   "metadata": {},
   "source": [
    "Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6372c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.compute import ComputeTarget, DataFactoryCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "# here importing DataFactory compute as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700bd4e",
   "metadata": {},
   "source": [
    "step 2 : Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1cba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a1ab4",
   "metadata": {},
   "source": [
    "Register Datastores and create DataReferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a13780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here using azure blob storage as default \n",
    "from azureml.exceptions import UserErrorException\n",
    "\n",
    "blob_datastore_name='MyBlobDatastore'\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME_62\", \"<my-account-name>\") # Storage account name\n",
    "container_name=os.getenv(\"BLOB_CONTAINER_62\", \"<my-container-name>\") # Name of Azure blob container\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY_62\", \"<my-account-key>\") # Storage account key\n",
    "\n",
    "try:\n",
    "    blob_datastore = Datastore.get(ws, blob_datastore_name)\n",
    "    print(\"Found Blob Datastore with name: %s\" % blob_datastore_name)\n",
    "except UserErrorException:\n",
    "    blob_datastore = Datastore.register_azure_blob_container(\n",
    "        workspace=ws,\n",
    "        datastore_name=blob_datastore_name,\n",
    "        account_name=account_name, # Storage account name\n",
    "        container_name=container_name, # Name of Azure blob container\n",
    "        account_key=account_key) # Storage account key\n",
    "    print(\"Registered blob datastore with name: %s\" % blob_datastore_name)\n",
    "\n",
    "blob_data_ref = DataReference(\n",
    "    datastore=blob_datastore,\n",
    "    data_reference_name=\"blob_test_data\",\n",
    "    path_on_datastore=\"testdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3d3e2",
   "metadata": {},
   "source": [
    "Step 3 : If we want to move data from/to blob store to/from Azure SQL database then use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_datastore_name=\"MySqlDatastore\"\n",
    "server_name=os.getenv(\"SQL_SERVERNAME_62\", \"<my-server-name>\") # Name of SQL server\n",
    "database_name=os.getenv(\"SQL_DATBASENAME_62\", \"<my-database-name>\") # Name of SQL database\n",
    "client_id=os.getenv(\"SQL_CLIENTNAME_62\", \"<my-client-id>\") # client id of service principal with permissions to access database\n",
    "client_secret=os.getenv(\"SQL_CLIENTSECRET_62\", \"<my-client-secret>\") # the secret of service principal\n",
    "tenant_id=os.getenv(\"SQL_TENANTID_62\", \"<my-tenant-id>\") # tenant id of service principal\n",
    "\n",
    "try:\n",
    "    sql_datastore = Datastore.get(ws, sql_datastore_name)\n",
    "    print(\"Found sql database datastore with name: %s\" % sql_datastore_name)\n",
    "except UserErrorException:\n",
    "    sql_datastore = Datastore.register_azure_sql_database(\n",
    "        workspace=ws,\n",
    "        datastore_name=sql_datastore_name,\n",
    "        server_name=server_name,\n",
    "        database_name=database_name,\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        tenant_id=tenant_id)\n",
    "    print(\"Registered sql databse datastore with name: %s\" % sql_datastore_name)\n",
    "\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "\n",
    "sql_query_data_ref = SqlDataReference(\n",
    "    datastore=sql_datastore,\n",
    "    data_reference_name=\"sql_query_data_ref\",\n",
    "    sql_query=\"select top 1 * from TestData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c22f86",
   "metadata": {},
   "source": [
    "DataTransferStep is used to transfer data between Azure Blob, Azure Data Lake Store, and Azure SQL database.\n",
    "\n",
    "name: Name of module\n",
    "source_data_reference: Input connection that serves as source of data transfer operation.\n",
    "destination_data_reference: Input connection that serves as destination of data transfer operation.\n",
    "compute_target: Azure Data Factory to use for transferring data.\n",
    "allow_reuse: Whether the step should reuse results of previous DataTransferStep when run with same inputs. Set as False to force data to be transferred again.\n",
    "Optional arguments to explicitly specify whether a path corresponds to a file or a directory. These are useful when storage contains both file and directory with the same name or when creating a new destination path.\n",
    "\n",
    "source_reference_type: An optional string specifying the type of source_data_reference. Possible values include: 'file', 'directory'. When not specified, we use the type of existing path or directory if it's a new path.\n",
    "destination_reference_type: An optional string specifying the type of destination_data_reference. Possible values include: 'file', 'directory'. When not specified, we use the type of existing path or directory if it's a new path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21edc0d9",
   "metadata": {},
   "source": [
    "### Setup Data Factory Account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factory_name = 'adftest'\n",
    "\n",
    "def get_or_create_data_factory(workspace, factory_name):\n",
    "    try:\n",
    "        return DataFactoryCompute(workspace, factory_name)\n",
    "    except ComputeTargetException as e:\n",
    "        if 'ComputeTargetNotFound' in e.message:\n",
    "            print('Data factory not found, creating...')\n",
    "            provisioning_config = DataFactoryCompute.provisioning_configuration()\n",
    "            data_factory = ComputeTarget.create(workspace, factory_name, provisioning_config)\n",
    "            data_factory.wait_for_completion()\n",
    "            return data_factory\n",
    "        else:\n",
    "            raise e\n",
    "            \n",
    "data_factory_compute = get_or_create_data_factory(ws, data_factory_name)\n",
    "\n",
    "print(\"Setup Azure Data Factory account complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9a0a1",
   "metadata": {},
   "source": [
    "### Create a DataTransferStep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28217e13",
   "metadata": {},
   "source": [
    " Step 4 : Transfer data from SQL to blob store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_sql_to_blob = DataTransferStep(\n",
    "    name=\"transfer_sql_to_blob\",\n",
    "    source_data_reference=sql_query_data_ref,\n",
    "    destination_data_reference=blob_data_ref,\n",
    "    compute_target=data_factory_compute,\n",
    "    destination_reference_type='file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde02cc",
   "metadata": {},
   "source": [
    "Build and Submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_01 = Pipeline(\n",
    "    description=\"data_transfer_01\",\n",
    "    workspace=ws,\n",
    "    steps=[transfer_sql_to_blob])\n",
    "\n",
    "pipeline_run_01 = Experiment(ws, \"Data_Transfer_example_01\").submit(pipeline_01)\n",
    "pipeline_run_01.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc309f",
   "metadata": {},
   "source": [
    "View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e378d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run_01).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8e656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
