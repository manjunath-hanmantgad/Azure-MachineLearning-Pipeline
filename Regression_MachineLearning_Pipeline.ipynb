{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49163d6",
   "metadata": {},
   "source": [
    "After creating configuration notebook.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3dcf5",
   "metadata": {},
   "source": [
    "#### Prepare data for regression modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2772f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use open dataset to get the dataset\n",
    "# pip install azureml-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version is :\" , azureml.core.VERSION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "from azureml.opendatasets import NycTlcGreen, NycTlcYellow\n",
    "#The Open Datasets package contains a class representing each data source (NycTlcGreen and NycTlcYellow) to easily filter date parameters before downloading.\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedata import relativedata\n",
    "\n",
    "green_df_raw = pd.DataFrame([]) # init a dataframe for appending data which is downloaded\n",
    "# take one month of data\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "# number of months taken is 1\n",
    "number_of_months = 1\n",
    "sample_size = 5000 # data taken out of 5000 samples; to avoid memory issues\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_green = NycTlcGreen(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    green_df_raw = green_df_raw.append(temp_df_green.sample(sample_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf037c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_df_raw = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_yellow = NycTlcYellow(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    yellow_df_raw = yellow_df_raw.append(temp_df_yellow.sample(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display data\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "display(green_df_raw.head(20))\n",
    "display(yellow_df_raw.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ab88e",
   "metadata": {},
   "source": [
    "Download data locally and then upload to Azure Blob.to save the dave in the default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataDir = \"data\"\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "greenDir = dataDir + \"/green\"\n",
    "yellowDir = dataDir + \"/yellow\"\n",
    "\n",
    "if not os.path.exists(greenDir):\n",
    "    os.mkdir(greenDir)\n",
    "    \n",
    "if not os.path.exists(yelloDir):\n",
    "    os.mkdir(yelloDir)\n",
    "    \n",
    "greenTaxiData = greenDir + \"/unprepared.parquet\"\n",
    "yellowTaxiData = yelloDir + \"/unprepared.parquet\"\n",
    "\n",
    "green_df_raw.to_csv(greenTaxiData, index=False)\n",
    "yellow_df_raw.to_csv(yellowTaxiData, index=False)\n",
    "\n",
    "print(\"Data written to local folder.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa5464",
   "metadata": {},
   "source": [
    "Create workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54398420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace: \" + ws.name, \"Region: \" + ws.location, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([greenTaxiData], \n",
    "                           target_path = 'green', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "default_store.upload_files([yellowTaxiData], \n",
    "                           target_path = 'yellow', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload calls completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97bc55",
   "metadata": {},
   "source": [
    "Create dataset\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd623cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "green_taxi_data = Dataset.Tabular.from_delimited_files(default_store.path('green/unprepared.parquet'))\n",
    "yellow_taxi_data = Dataset.Tabular.from_delimited_files(default_store.path('yellow/unprepared.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf75c6",
   "metadata": {},
   "source": [
    "Register dataset\n",
    "\n",
    "Register the taxi datasets with the workspace so that you can reuse them in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_data = green_taxi_data.register(ws, 'green_taxi_data')\n",
    "yellow_taxi_data = yellow_taxi_data.register(ws, 'yellow_taxi_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290887a",
   "metadata": {},
   "source": [
    "Create Compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6bca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836f4c6",
   "metadata": {},
   "source": [
    "Define RunConfig for the compute\n",
    "This is to load all dependencies as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d20d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn'], \n",
    "    pip_packages=['azureml-sdk[automl]', 'pyarrow'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8046727",
   "metadata": {},
   "source": [
    "Prepare data\n",
    "Using pandas,run data transformations to combine the above created 2 datasets (yellow and green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8136e5",
   "metadata": {},
   "source": [
    "creating a separate step for each transformation as this allows us to reuse the steps and saves us from running all over again in case of any change. We will keep data preparation scripts in one subfolder and training scripts in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns\n",
    "\n",
    "display(green_df_raw.columns)\n",
    "display(yellow_df_raw.columns)\n",
    "\n",
    "# useful columns needed for the Azure Machine Learning NYC Taxi tutorial\n",
    "useful_columns = str([\"cost\", \"distance\", \"dropoff_datetime\", \"dropoff_latitude\", \n",
    "                      \"dropoff_longitude\", \"passengers\", \"pickup_datetime\", \n",
    "                      \"pickup_latitude\", \"pickup_longitude\", \"store_forward\", \"vendor\"]).replace(\",\", \";\")\n",
    "\n",
    "print(\"Useful columns defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987ce47",
   "metadata": {},
   "source": [
    "Cleanse Green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts/prepdata'\n",
    "\n",
    "# rename columns\n",
    "green_columns = str({ \n",
    "    \"vendorID\": \"vendor\",\n",
    "    \"lpepPickupDatetime\": \"pickup_datetime\",\n",
    "    \"lpepDropoffDatetime\": \"dropoff_datetime\",\n",
    "    \"storeAndFwdFlag\": \"store_forward\",\n",
    "    \"pickupLongitude\": \"pickup_longitude\",\n",
    "    \"pickupLatitude\": \"pickup_latitude\",\n",
    "    \"dropoffLongitude\": \"dropoff_longitude\",\n",
    "    \"dropoffLatitude\": \"dropoff_latitude\",\n",
    "    \"passengerCount\": \"passengers\",\n",
    "    \"fareAmount\": \"cost\",\n",
    "    \"tripDistance\": \"distance\"\n",
    "}).replace(\",\", \";\")\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleansed_green_data = PipelineData(\"cleansed_green_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStepGreen = PythonScriptStep(\n",
    "    name=\"Cleanse Green Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--useful_columns\", useful_columns,\n",
    "               \"--columns\", green_columns,\n",
    "               \"--output_cleanse\", cleansed_green_data],\n",
    "    inputs=[green_taxi_data.as_named_input('raw_data')],\n",
    "    outputs=[cleansed_green_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepGreen created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a49b9",
   "metadata": {},
   "source": [
    "Cleanse Yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39260d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_columns = str({\n",
    "    \"vendorID\": \"vendor\",\n",
    "    \"tpepPickupDateTime\": \"pickup_datetime\",\n",
    "    \"tpepDropoffDateTime\": \"dropoff_datetime\",\n",
    "    \"storeAndFwdFlag\": \"store_forward\",\n",
    "    \"startLon\": \"pickup_longitude\",\n",
    "    \"startLat\": \"pickup_latitude\",\n",
    "    \"endLon\": \"dropoff_longitude\",\n",
    "    \"endLat\": \"dropoff_latitude\",\n",
    "    \"passengerCount\": \"passengers\",\n",
    "    \"fareAmount\": \"cost\",\n",
    "    \"tripDistance\": \"distance\"\n",
    "}).replace(\",\", \";\")\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleansed_yellow_data = PipelineData(\"cleansed_yellow_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStepYellow = PythonScriptStep(\n",
    "    name=\"Cleanse Yellow Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--useful_columns\", useful_columns,\n",
    "               \"--columns\", yellow_columns,\n",
    "               \"--output_cleanse\", cleansed_yellow_data],\n",
    "    inputs=[yellow_taxi_data.as_named_input('raw_data')],\n",
    "    outputs=[cleansed_yellow_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepYellow created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb083b9e",
   "metadata": {},
   "source": [
    "### Merge cleansed Green and Yellow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after merging step\n",
    "merged_data = PipelineData(\"merged_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Merge script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# merging step creation\n",
    "# See the merge.py for details about input and output\n",
    "mergingStep = PythonScriptStep(\n",
    "    name=\"Merge Taxi Data\",\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--output_merge\", merged_data],\n",
    "    inputs=[cleansed_green_data.parse_parquet_files(),\n",
    "            cleansed_yellow_data.parse_parquet_files()],\n",
    "    outputs=[merged_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"mergingStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33831a9",
   "metadata": {},
   "source": [
    "**Filter data**\n",
    "This step filters out coordinates for locations that are outside the city border. We use a TypeConverter object to change the latitude and longitude fields to decimal type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after merging step\n",
    "filtered_data = PipelineData(\"filtered_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Filter script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# filter step creation\n",
    "# See the filter.py for details about input and output\n",
    "filterStep = PythonScriptStep(\n",
    "    name=\"Filter Taxi Data\",\n",
    "    script_name=\"filter.py\", \n",
    "    arguments=[\"--output_filter\", filtered_data],\n",
    "    inputs=[merged_data.parse_parquet_files()],\n",
    "    outputs=[filtered_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"FilterStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24373f",
   "metadata": {},
   "source": [
    "**Normalize data**\n",
    "In this step, we split the pickup and dropoff datetime values into the respective date and time columns and then we rename the columns to use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after normalize step\n",
    "normalized_data = PipelineData(\"normalized_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Normalize script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# normalize step creation\n",
    "# See the normalize.py for details about input and output\n",
    "normalizeStep = PythonScriptStep(\n",
    "    name=\"Normalize Taxi Data\",\n",
    "    script_name=\"normalize.py\", \n",
    "    arguments=[\"--output_normalize\", normalized_data],\n",
    "    inputs=[filtered_data.parse_parquet_files()],\n",
    "    outputs=[normalized_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"normalizeStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f92626",
   "metadata": {},
   "source": [
    "**Transform data**\n",
    "\n",
    "Split the pickup and dropoff date further into the day of the week, day of the month, and month values.\n",
    "To get the day of the week value, uses the derive_column_by_example() function. The function takes an array parameter of example objects that define the input data, and the preferred output. The function automatically determines the preferred transformation. For the pickup and dropoff time columns, split the time into the hour, minute, and second by using the split_column_by_example() function with no example parameter.\n",
    "After new features are generated, use the drop_columns() function to delete the original fields as the newly generated features are preferred.\n",
    "Rename the rest of the fields to use meaningful descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d57bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after transform step\n",
    "transformed_data = PipelineData(\"transformed_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Transform script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# transform step creation\n",
    "# See the transform.py for details about input and output\n",
    "transformStep = PythonScriptStep(\n",
    "    name=\"Transform Taxi Data\",\n",
    "    script_name=\"transform.py\", \n",
    "    arguments=[\"--output_transform\", transformed_data],\n",
    "    inputs=[normalized_data.parse_parquet_files()],\n",
    "    outputs=[transformed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"transformStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c0844",
   "metadata": {},
   "source": [
    "Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_folder = './scripts/trainmodel'\n",
    "\n",
    "# train and test splits output\n",
    "output_split_train = PipelineData(\"output_split_train\", datastore=default_store).as_dataset()\n",
    "output_split_test = PipelineData(\"output_split_test\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Data spilt script is in {}.'.format(os.path.realpath(train_model_folder)))\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "testTrainSplitStep = PythonScriptStep(\n",
    "    name=\"Train Test Data Split\",\n",
    "    script_name=\"train_test_split.py\", \n",
    "    arguments=[\"--output_split_train\", output_split_train,\n",
    "               \"--output_split_test\", output_split_test],\n",
    "    inputs=[transformed_data.parse_parquet_files()],\n",
    "    outputs=[output_split_train, output_split_test],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=train_model_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"testTrainSplitStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d931e",
   "metadata": {},
   "source": [
    "This completes the datapreparation part. \n",
    "Next is model training. For this we are using AutoML to  train the model with different algorithms ( voting and stacking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14190a",
   "metadata": {},
   "source": [
    "For automl using automl step class.\n",
    "\n",
    "**pip install azureml -sdk[automl]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83103e6f",
   "metadata": {},
   "source": [
    "### Automatically train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00cf87",
   "metadata": {},
   "source": [
    "Step 1: Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f52cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'NYCTaxi_Tutorial_Pipelines')\n",
    "\n",
    "print(\"Experiment created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab533c10",
   "metadata": {},
   "source": [
    "Step 2 : Define settings for autogeneration and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e47a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your training data and the type of model\n",
    "\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\" : 10,\n",
    "    \"iterations\" : 25,\n",
    "    \"primary_metric\" : 'spearman_correlation',\n",
    "    \"n_cross_validations\": 5\n",
    "}\n",
    "\n",
    "training_dataset = output_split_train.parse_parquet_files().keep_columns(['pickup_weekday','pickup_hour', 'distance','passengers', 'vendor', 'cost'])\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             debug_log = 'automated_ml_errors.log',\n",
    "                             path = train_model_folder,\n",
    "                             compute_target = aml_compute,\n",
    "                             featurization = 'auto',\n",
    "                             training_data = training_dataset,\n",
    "                             label_column_name = 'cost',\n",
    "                             **automl_settings)\n",
    "                             \n",
    "print(\"AutoML config created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a342e52",
   "metadata": {},
   "source": [
    "Step 3 : Define AutoMLStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "trainWithAutomlStep = AutoMLStep(name='AutoML_Regression',\n",
    "                                 automl_config=automl_config,\n",
    "                                 allow_reuse=True)\n",
    "print(\"trainWithAutomlStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e35db8c",
   "metadata": {},
   "source": [
    "Step 4 : Build and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline_steps = [trainWithAutomlStep]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pipeline progress\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20843bf5",
   "metadata": {},
   "source": [
    "Step 5: Explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we proceed we need to wait for the run to complete.\n",
    "pipeline_run.wait_for_completion(show_output=False)\n",
    "\n",
    "# functions to download output to local and fetch as dataframe\n",
    "def get_download_path(download_path, output_name):\n",
    "    output_folder = os.listdir(download_path + '/azureml')[0]\n",
    "    path =  download_path + '/azureml/' + output_folder + '/' + output_name\n",
    "    return path\n",
    "\n",
    "def fetch_df(current_step, output_name):\n",
    "    output_data = current_step.get_output_data(output_name)    \n",
    "    download_path = './outputs/' + output_name\n",
    "    output_data.download(download_path, overwrite=True)\n",
    "    df_path = get_download_path(download_path, output_name) + '/processed.parquet'\n",
    "    return pd.read_parquet(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87453e",
   "metadata": {},
   "source": [
    "Step 6: View cleansed taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ceb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_cleanse_step = pipeline_run.find_step_run(cleansingStepGreen.name)[0]\n",
    "yellow_cleanse_step = pipeline_run.find_step_run(cleansingStepYellow.name)[0]\n",
    "\n",
    "cleansed_green_df = fetch_df(green_cleanse_step, cleansed_green_data.name)\n",
    "cleansed_yellow_df = fetch_df(yellow_cleanse_step, cleansed_yellow_data.name)\n",
    "\n",
    "display(cleansed_green_df.head(5))\n",
    "display(cleansed_yellow_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58681380",
   "metadata": {},
   "source": [
    "Step 7: View combined taxi data profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_step = pipeline_run.find_step_run(mergingStep.name)[0]\n",
    "combined_df = fetch_df(merge_step, merged_data.name)\n",
    "\n",
    "display(combined_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view filtered taxi data\n",
    "filter_step = pipeline_run.find_step_run(filterStep.name)[0]\n",
    "filtered_df = fetch_df(filter_step, filtered_data.name)\n",
    "\n",
    "display(filtered_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view normalized taxi data\n",
    "\n",
    "normalize_step = pipeline_run.find_step_run(normalizeStep.name)[0]\n",
    "normalized_df = fetch_df(normalize_step, normalized_data.name)\n",
    "\n",
    "display(normalized_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View transformed taxi data\n",
    "\n",
    "transform_step = pipeline_run.find_step_run(transformStep.name)[0]\n",
    "transformed_df = fetch_df(transform_step, transformed_data.name)\n",
    "\n",
    "display(transformed_df.describe())\n",
    "display(transformed_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6f77f",
   "metadata": {},
   "source": [
    "Step 8 : View training data used by AutoML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_step = pipeline_run.find_step_run(testTrainSplitStep.name)[0]\n",
    "train_split = fetch_df(split_step, output_split_train.name)\n",
    "\n",
    "display(train_split.describe())\n",
    "display(train_split.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e502f",
   "metadata": {},
   "source": [
    "Step 9 : View the details of the AutoML run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314124d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.run import AutoMLRun\n",
    "#from azureml.widgets import RunDetails\n",
    "\n",
    "# workaround to get the automl run as its the last step in the pipeline \n",
    "# and get_steps() returns the steps from latest to first\n",
    "\n",
    "for step in pipeline_run.get_steps():\n",
    "    automl_step_run_id = step.id\n",
    "    print(step.name)\n",
    "    print(automl_step_run_id)\n",
    "    break\n",
    "\n",
    "automl_run = AutoMLRun(experiment = experiment, run_id=automl_step_run_id)\n",
    "#RunDetails(automl_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81c75a",
   "metadata": {},
   "source": [
    "Step 10 : Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46fa626",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = automl_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a11d35",
   "metadata": {},
   "source": [
    "### Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data \n",
    "\n",
    "split_step = pipeline_run.find_step_run(testTrainSplitStep.name)[0]\n",
    "\n",
    "x_test = fetch_df(split_step, output_split_test.name)[['distance','passengers', 'vendor','pickup_weekday','pickup_hour']]\n",
    "y_test = fetch_df(split_step, output_split_test.name)[['cost']]\n",
    "\n",
    "display(x_test.head(5))\n",
    "display(y_test.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76239a65",
   "metadata": {},
   "source": [
    "Test the best fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e341cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = fitted_model.predict(x_test)\n",
    "y_actual =  y_test.values.tolist()\n",
    "display(pd.DataFrame({'Actual':y_actual, 'Predicted':y_predict}).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "distance_vals = [x[0] for x in x_test.values]\n",
    "\n",
    "ax1.scatter(distance_vals[:100], y_predict[:100], s=18, c='b', marker=\"s\", label='Predicted')\n",
    "ax1.scatter(distance_vals[:100], y_actual[:100], s=18, c='r', marker=\"o\", label='Actual')\n",
    "ax1.set_xlabel('distance (mi)')\n",
    "ax1.set_title('Predicted and Actual Cost/Distance')\n",
    "ax1.set_ylabel('Cost ($)')\n",
    "\n",
    "plt.legend(loc='upper left', prop={'size': 12})\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63133c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
